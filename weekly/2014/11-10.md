Thursday, 13 November 2014
==========================
I need to download data from a remote mysql server to a local server. I can only connect to the remote server via a ssh tunnel. How can I do that? 

I have ssh set up; I generated the key, copied both private and public keys to ~/.ssh, created a config file to keep the IdentityFile commands (so I can use multiple keys). The public key has been added to on the remote server.

I can successfully ssh to the remote system, and can successfully run rsync. Now, how can I set up Laravel to remotely connect? I'll send a question to laravel peeps...

I need to download data from a remote mysql server to a local server. I can only connect to the remote server via a ssh tunnel. Any ideas on how to do that?

I have set up config/remote.php as follows:

'website' => array(
    'host' => 'remote.site',
    'username'  => 'username',
    'key'       => 'path/to/private/key',
    'keyphrase' => '',    // for now, no keyphrase
),
This works correctly. I tested it with:

SSH::into('website')->run([
    'touch foo.txt'
]);
I can ssh to the server (and delete the foo.txt file that was just created ;-) ). While there, I can open the database. So far, I haven't been able to figure out how to connect to it from Laravel. I've tried a couple of things in database.php. The basic setup is this:

'attempt1' => array(
    'driver'    => 'mysql',
    'host'      => 'remote.site',
    'database'  => 'database_name',
    'username'  => 'user',
    'password'  => 'pass',
    'charset'   => 'utf8',
    'collation' => 'utf8_unicode_ci',
    'prefix'    => '',
),


Other remote things work like this:

    ssh -f user@ssh.example.com -L 3307:mysql1.example.com:3306 -N
    mysql -h 127.0.0.1 -P 3307

I can't seem to get that to work, though. When I tried:

    $ ssh -f user@domain -L 3307:test.foo:3306 -N
    $ mysql -h 127.0.0.1 -P 3307

... it gave me:

    $ channel 2: open failed: administratively prohibited: open failed

When I try:

    $ ssh user@domain -L 3306:test.foo:3307 -N

I get:

    bind: Operation not permitted



Friday, 14 November 2014
========================
My best idea at the moment is to run through this process:

    • Run mysql –e to save records to a table (a new "transfer" table)
    • Run mysqldump to save the transfer table to a dump file
    • Move the dump file to our local server
    • Load the data from the dump file into a transfer table on our local server
    • Import data from the transfer table to the database
    • Run mysql –e to delete records from the table on the website.

I'm trying to pull data from a drupal installation... 

This is tricky....

I'm having a hard time saving a string that mysql -e can run. I'm trying to escape a backtick, and have not been able to do it. 

I can leave a file on the server, and run it...

    DELETE FROM transfer;

    INSERT INTO transfer (sid,`key`,`data`) 
        SELECT d.sid, 
            c.form_key, 
            d.data
        FROM webform_submitted_data d
        LEFT JOIN webform_component c 
            ON d.cid = c.cid
            AND d.nid = c.nid
        WHERE c.nid=995
        AND NOT c.type='file';

    INSERT INTO transfer (sid,`key`,`data`)
        SELECT d.sid,
            c.form_key,
            f.filepath
        FROM webform_submitted_data d
        LEFT JOIN webform_component c 
            ON d.cid = c.cid
            AND d.nid = c.nid
        LEFT JOIN files f 
            ON d.data = f.fid
        WHERE c.nid=995
        AND c.type='file';

    INSERT INTO transfer (sid,`key`,`data`)
        SELECT sid,
            'create_date',
            FROM_UNIXTIME(submitted)
        FROM webform_submissions
        WHERE nid=995;

I don't like hardcoding the form ID number, but I don't see another option at the moment. This leaves me with two nearly identical files (other than the form ID number).

Dump the resulting transfer file with this:

    mysqldump my_database -u user -ppassword transfer > path/to/dump.sql

Then, I can run this:

    SSH::into('website')->run([
        'mysql my_database < path/to/prepare_online_application_transfer.sql',
        'mysqldump my_database transfer > path/to/dump.sql',
    ]);

    SSH::into('website')
        ->get(
            '/path/to/dump.sql', 
            storage_path().'/transfer/dump.sql'
        );

    DB::unprepared(file_get_contents(storage_path().'/transfer/dump.sql'));

After the data is loaded, I can use this to delete the old records:

    SSH::into('website')->run([
        'rm /path/to/dump.sql',
        'mysql my_database -e "DELETE FROM transfer",
        'mysql my_database -e "DELETE FROM webform_submissions WHERE sid IN ()",
        'mysql my_database -e "DELETE FROM webform_submitted_data WHERE sid IN ()",
        'mysql my_database -e "DELETE FROM files WHERE fid IN ()",
    ]);




Sunday, 16 November 2014
========================
I have several tables that have an update_date field, but no created_at field. How can I do that with Laravel?

I can disable the created_at field like this:

public function setCreatedAtAttribute($value) {}

